{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9b7156-1656-4614-adb0-9dcaf472678b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pairwise registration evaluation with meanshift filter denoising for both affine and deformable registration.\n",
    "\"\"\"\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "sys.path.append('../spami/')\n",
    "\n",
    "import json\n",
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from spami.reg_graph import RegGraph\n",
    "from spami.utils.utils import create_if_not_exists, clean_configs\n",
    "from spami.utils.plot import plot_registration_summary\n",
    "from spami.utils.image_utils import get_symmetric_padding_for_sections\n",
    "from spami.utils.metrics import compute_tre\n",
    "\n",
    "        \n",
    "\n",
    "def get_core_names():\n",
    "    return [x.split('.')[0] for x in os.listdir('../spami/configs/cores_hr/')]\n",
    "\n",
    "\n",
    "def get_default_args():\n",
    "    return {\n",
    "        'kernel_divider': 100,\n",
    "        'resolution': 1024,\n",
    "        'use_segmentation_masks': True,    \n",
    "        'output_dir': 'save_directories/temp_pairwise/',\n",
    "        'tmp_dir': 'save_directories/temp_pairwise/tmp',\n",
    "        'cleanup_temporary_directories': False,\n",
    "        'remove_temp_directory': False,\n",
    "        'cost_fun': 'WNCC',\n",
    "        'ia': 'ia-com-init',\n",
    "        'affine_use_denoising': True,\n",
    "        'deformable_use_denoising': False\n",
    "}\n",
    "\n",
    "\n",
    "def main():\n",
    "    skip_processed_cores = True\n",
    "    save_plots = False\n",
    "    root_dir = 'save_directories/pairwise_analysis/greedy_f_hist'\n",
    "    plot_root_dir = 'save_directories/pairwise_analysis/greedy_f_hist_plots'\n",
    "    create_if_not_exists(plot_root_dir)\n",
    "    if not os.path.exists(root_dir):\n",
    "        os.mkdir(root_dir)\n",
    "    core_names = get_core_names()\n",
    "    df_all = pd.DataFrame()\n",
    "    for core_name in core_names:\n",
    "        print(f'Working on core: {core_name}')       \n",
    "        target_dir = join(root_dir, core_name)\n",
    "        if os.path.exists(target_dir) and skip_processed_cores:\n",
    "            print('Already processed!')\n",
    "            continue\n",
    "        create_if_not_exists(target_dir)\n",
    "        config_path = os.path.join('configs/cores_hr/', core_name + '.json')\n",
    "        \n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "            config = clean_configs(config)\n",
    "            # config = filter_node_ids(config, section_ids)\n",
    "        graph = RegGraph.from_config(config)        \n",
    "        df_core = pd.DataFrame()\n",
    "        section_ids = list(graph.sections)\n",
    "        # paddings = get_symmetric_padding_for_sections([graph.sections[x] for x in section_ids])\n",
    "        \n",
    "        for i in range(len(section_ids)-1):\n",
    "            source_idx = section_ids[i]\n",
    "            target_idx = section_ids[i+1]\n",
    "            source_section = graph.sections[source_idx].copy()\n",
    "            target_section = graph.sections[target_idx].copy()\n",
    "            if source_section.landmarks is None or target_section.landmarks is None:\n",
    "                continue\n",
    "            print(f'Now processing: {source_idx} and {target_idx}')\n",
    "            sub_dir = join(target_dir, f'{source_idx}_{target_idx}')\n",
    "            create_if_not_exists(sub_dir)\n",
    "            warp_dir = join(sub_dir, 'warped_section')\n",
    "            create_if_not_exists(warp_dir)\n",
    "            args = get_default_args()\n",
    "            registration_result = graph.default_registerer.coregister_images(moving_img=source_section.image.data,\n",
    "                                                                             fixed_img=target_section.image.data,\n",
    "                                                                             moving_img_mask=source_section.segmentation_mask.data,\n",
    "                                                                             fixed_img_mask=target_section.segmentation_mask.data,\n",
    "                                                                             args=args)  \n",
    "            warped_section = source_section.warp(graph.default_registerer, registration_result, args) # Warp section here\n",
    "            warped_section.store(warp_dir)\n",
    "            mean_rtre, median_rtre, mean_tre, median_tre = compute_tre(target_section, warped_section)\n",
    "            row = {\n",
    "                'core_name': core_name,\n",
    "                'fixed_section_id': target_idx,\n",
    "                'moving_section_id': source_idx,\n",
    "                'mean_rtre': mean_rtre,\n",
    "                'median_rtre': median_rtre,\n",
    "                'mean_tre': mean_tre,\n",
    "                'median_tre': median_tre\n",
    "            }\n",
    "            print(row)\n",
    "            row = pd.DataFrame(row, index=[0])\n",
    "            df_core = pd.concat([df_core, row]).reset_index(drop=True)\n",
    "            if save_plots:\n",
    "                save_dir = join(plot_root_dir, core_name)\n",
    "                create_if_not_exists(save_dir)\n",
    "                save_path = join(save_dir, f'{source_idx}_{target_idx}.png')\n",
    "                plot_registration_summary(source_section, target_section, warped_section, save_path, with_landmarks=True)\n",
    "                                 \n",
    "        df_core.to_csv(join(target_dir, 'stats.csv'))\n",
    "        df_all = pd.concat([df_all, df_core]).reset_index(drop=True)\n",
    "        \n",
    "    df_all.to_csv(join(root_dir, 'stats.csv'))\n",
    "                    \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fded0ae-a17b-4020-87a4-aab2fe9e3d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on core: 048_01\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'configs/cores_hr/048_01.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m create_if_not_exists(target_dir)\n\u001b[1;32m     17\u001b[0m config_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs/cores_hr/\u001b[39m\u001b[38;5;124m'\u001b[39m, core_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     21\u001b[0m     config \u001b[38;5;241m=\u001b[39m clean_configs(config)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'configs/cores_hr/048_01.json'"
     ]
    }
   ],
   "source": [
    "skip_processed_cores = True\n",
    "save_plots = False\n",
    "root_dir = 'save_directories/pairwise_analysis/greedy_f_hist'\n",
    "plot_root_dir = 'save_directories/pairwise_analysis/greedy_f_hist_plots'\n",
    "create_if_not_exists(plot_root_dir)\n",
    "if not os.path.exists(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "core_names = get_core_names()\n",
    "df_all = pd.DataFrame()\n",
    "for core_name in core_names:\n",
    "    print(f'Working on core: {core_name}')       \n",
    "    target_dir = join(root_dir, core_name)\n",
    "    if os.path.exists(target_dir) and skip_processed_cores:\n",
    "        print('Already processed!')\n",
    "        continue\n",
    "    create_if_not_exists(target_dir)\n",
    "    config_path = os.path.join('configs/cores_hr/', core_name + '.json')\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "        config = clean_configs(config)\n",
    "        # config = filter_node_ids(config, section_ids)\n",
    "    graph = RegGraph.from_config(config)        \n",
    "    df_core = pd.DataFrame()\n",
    "    section_ids = list(graph.sections)\n",
    "    # paddings = get_symmetric_padding_for_sections([graph.sections[x] for x in section_ids])\n",
    "\n",
    "    for i in range(len(section_ids)-1):\n",
    "        source_idx = section_ids[i]\n",
    "        target_idx = section_ids[i+1]\n",
    "        source_section = graph.sections[source_idx].copy()\n",
    "        target_section = graph.sections[target_idx].copy()\n",
    "        break\n",
    "        if source_section.landmarks is None or target_section.landmarks is None:\n",
    "            continue\n",
    "        print(f'Now processing: {source_idx} and {target_idx}')\n",
    "        sub_dir = join(target_dir, f'{source_idx}_{target_idx}')\n",
    "        create_if_not_exists(sub_dir)\n",
    "        warp_dir = join(sub_dir, 'warped_section')\n",
    "        create_if_not_exists(warp_dir)\n",
    "        args = get_default_args()\n",
    "        registration_result = graph.default_registerer.coregister_images(moving_img=source_section.image.data,\n",
    "                                                                         fixed_img=target_section.image.data,\n",
    "                                                                         moving_img_mask=source_section.segmentation_mask.data,\n",
    "                                                                         fixed_img_mask=target_section.segmentation_mask.data,\n",
    "                                                                         args=args)  \n",
    "        warped_section = source_section.warp(graph.default_registerer, registration_result, args) # Warp section here\n",
    "        warped_section.store(warp_dir)\n",
    "        mean_rtre, median_rtre, mean_tre, median_tre = compute_tre(target_section, warped_section)\n",
    "        row = {\n",
    "            'core_name': core_name,\n",
    "            'fixed_section_id': target_idx,\n",
    "            'moving_section_id': source_idx,\n",
    "            'mean_rtre': mean_rtre,\n",
    "            'median_rtre': median_rtre,\n",
    "            'mean_tre': mean_tre,\n",
    "            'median_tre': median_tre\n",
    "        }\n",
    "        print(row)\n",
    "        row = pd.DataFrame(row, index=[0])\n",
    "        df_core = pd.concat([df_core, row]).reset_index(drop=True)\n",
    "        if save_plots:\n",
    "            save_dir = join(plot_root_dir, core_name)\n",
    "            create_if_not_exists(save_dir)\n",
    "            save_path = join(save_dir, f'{source_idx}_{target_idx}.png')\n",
    "            plot_registration_summary(source_section, target_section, warped_section, save_path, with_landmarks=True)\n",
    "\n",
    "    df_core.to_csv(join(target_dir, 'stats.csv'))\n",
    "    df_all = pd.concat([df_all, df_core]).reset_index(drop=True)\n",
    "\n",
    "df_all.to_csv(join(root_dir, 'stats.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba361646-4f5a-4150-bb7d-8629f829f186",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on core: 048_01\n"
     ]
    }
   ],
   "source": [
    "skip_processed_cores = True\n",
    "save_plots = False\n",
    "root_dir = 'save_directories/pairwise_analysis/greedy_f_hist'\n",
    "if not os.path.exists(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "core_names = get_core_names()\n",
    "df_all = pd.DataFrame()\n",
    "for core_name in core_names:\n",
    "    print(f'Working on core: {core_name}')       \n",
    "    config_path = os.path.join('../spami/configs/cores_hr/', core_name + '.json')\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "        config = clean_configs(config)\n",
    "        # config = filter_node_ids(config, section_ids)\n",
    "    graph = RegGraph.from_config(config)        \n",
    "    df_core = pd.DataFrame()\n",
    "    section_ids = list(graph.sections)\n",
    "    # paddings = get_symmetric_padding_for_sections([graph.sections[x] for x in section_ids])\n",
    "\n",
    "    for i in range(len(section_ids)-1):\n",
    "        source_idx = section_ids[i]\n",
    "        target_idx = section_ids[i+1]\n",
    "        source_section = graph.sections[source_idx].copy()\n",
    "        target_section = graph.sections[target_idx].copy()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7551424a-7360-4701-8777-8f3dd8dd9d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'SURF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Create SURF Feature Detector object\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m surf \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSURF\u001b[49m()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Only features, whose hessian is larger than hessianThreshold are retained by the detector\u001b[39;00m\n\u001b[1;32m     11\u001b[0m surf\u001b[38;5;241m.\u001b[39mhessianThreshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'SURF'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = source_section.image.data.copy()\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Create SURF Feature Detector object\n",
    "surf = cv2.SURF()\n",
    "\n",
    "# Only features, whose hessian is larger than hessianThreshold are retained by the detector\n",
    "surf.hessianThreshold = 500\n",
    "keypoints, descriptors = surf.detectAndCompute(gray, None)\n",
    "print(\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich key points on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - SURF', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17afbd0e-9f61-4473-9ab2-a1a5de616432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'xfeatures2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m surf \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxfeatures2d\u001b[49m\u001b[38;5;241m.\u001b[39mSURF_create()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'xfeatures2d'"
     ]
    }
   ],
   "source": [
    "surf = cv2.xfeatures2d.SURF_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6dad7ef-cbaf-4749-b7d7-2039e97619a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6.0.66'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.version.opencv_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c593a-9102-4ec2-b635-ecdc5c596e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.xf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-wsi]",
   "language": "python",
   "name": "conda-env-.conda-wsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
